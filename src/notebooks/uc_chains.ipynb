{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xobu8QB8mu_l"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_vzyCRHF3Xn",
        "outputId": "8d3af069-413e-47d3-c178-039a969ad822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet langchain openai faiss-cpu tiktoken pypdf PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G80rr2ZHP7d",
        "outputId": "25ff0a08-071b-4987-8876-968e9113649c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/LLM/ulcerative_colitis\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "%cd drive/MyDrive/LLM/ulcerative_colitis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nGWwk_exH6Op"
      },
      "outputs": [],
      "source": [
        "import os, json, logging\n",
        "import os.path as osp\n",
        "import re\n",
        "import pandas as pd\n",
        "from abc import abstractmethod\n",
        "\n",
        "from typing import Any, Union, Tuple, Dict, Callable, List, Optional, Literal\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "from langchain.docstore.document import Document\n",
        "from chromadb.config import Settings\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import VectorStore, FAISS, Chroma, Pinecone\n",
        "import pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser, ListOutputParser\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n",
        "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain, _split_list_of_docs, _collapse_docs\n",
        "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.callbacks.manager import Callbacks\n",
        "from custom_parsers import DrugOutput, DrugParser\n",
        "\n",
        "import config\n",
        "from config import MAIN_DIR, DATA_DIR, ARTIFACT_DIR, DOCUMENT_SOURCE\n",
        "\n",
        "from shutil import rmtree\n",
        "from utils import load_single_document, load_documents, convert_json_to_documents, convert_csv_to_documents\n",
        "import yaml\n",
        "\n",
        "from pydantic import root_validator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l-VsrU--JBBK"
      },
      "outputs": [],
      "source": [
        "PROJECT = \"uc\"\n",
        "\n",
        "with open(osp.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
        "    keys = json.load(f)\n",
        "\n",
        "OPENAI_KEY = keys[\"OPENAI_API_KEY\"]\n",
        "SOURCE_DATA = os.path.join(DOCUMENT_SOURCE, PROJECT)\n",
        "EMBSTORE_DIR = os.path.join(config.EMBSTORE_DIR, PROJECT, \"faiss\", \"text-embedding-ada-002\")\n",
        "\n",
        "EXCLUDE_DICT = {\n",
        "    \"agrawal.pdf\": [13, 14, 15, 16, 17, 18],\n",
        "    \"PIIS1542356520300446.pdf\": [12, 13, 14, 15, 16, 17, 18],\n",
        "    \"gutjnl-2021-326390R2 CLEAN.pdf\": [0, 2, 31, 32, 33, 34, 35, 36,\n",
        "                                       37, 38, 39, 40, 41, 42, 43, 44, 45]\\\n",
        "                                        + list(range(3, 31)),\n",
        "    \"otad009.pdf\": [15, 16],\n",
        "    \"1-s2.0-S2468125321003770-main.pdf\": [9],\n",
        "    \"juillerat 2022.pdf\": [6, 7, 8],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "piuTlxLNJTVC"
      },
      "outputs": [],
      "source": [
        "LOGGER = logging.getLogger()\n",
        "\n",
        "log_path = os.path.join(MAIN_DIR, \"log\", \"logfile.txt\")\n",
        "file_handler = logging.FileHandler(\n",
        "    filename=log_path)\n",
        "\n",
        "formatter = logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "LOGGER.setLevel(logging.INFO)\n",
        "LOGGER.addHandler(file_handler)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B_Uslox4t9d8"
      },
      "source": [
        "# User-defined Functions (UDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "akNXqtFmt88J"
      },
      "outputs": [],
      "source": [
        "def convert_csv_to_documents(table_info: Dict, concatenate_rows: bool = True) -> List[Document]:\n",
        "    \"\"\"Convert a dictionary containing table information into list of Documents\n",
        "\n",
        "    Args:\n",
        "        table_info (Dict): Dictionary containing .csv table information\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of rows inside the table\n",
        "    \"\"\"\n",
        "    assert table_info[\"mode\"] == \"table\" and table_info[\"filename\"].endswith(\".csv\")\n",
        "    rows = load_single_document(os.path.join(MAIN_DIR, table_info[\"filename\"]))\n",
        "    documents = []\n",
        "    table_content = table_info[\"description\"] + \"\\n\\n\"\n",
        "    for row in rows:\n",
        "        if concatenate_rows:\n",
        "            table_content += row.page_content + \"\\n\\n\"\n",
        "            table_doc = Document(\n",
        "                page_content=table_content,\n",
        "                metadata=table_info[\"metadata\"]\n",
        "            )\n",
        "        else:\n",
        "            row_no = row.metadata[\"row\"]\n",
        "            metadata = {k: v for k, v in table_info[\"metadata\"].items()}\n",
        "            metadata[\"row\"] = row_no\n",
        "            metadata[\"modal\"] = table_info[\"mode\"]\n",
        "            row.page_content = table_info[\"description\"] + \":\" + row.page_content\n",
        "            row.metadata = metadata\n",
        "            documents.append(row)\n",
        "            \n",
        "    if concatenate_rows:\n",
        "        documents.append(table_doc)\n",
        "    \n",
        "    return documents\n",
        "\n",
        "\n",
        "def generate_vectorstore(\n",
        "    embeddings: Callable,\n",
        "    source_directory: Optional[str] = None,\n",
        "    output_directory: str = \"./vectorstore\",\n",
        "    emb_store_type: str = \"faiss\",\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 250,\n",
        "    exclude_pages: Optional[Dict] = None,\n",
        "    pinecone_idx_name: Optional[str] = None,\n",
        "    additional_docs: Optional[List] = None,\n",
        "    key_path: Optional[str] = os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"),\n",
        ") -> VectorStore:\n",
        "    \"\"\"Generate New Vector Index Database\n",
        "\n",
        "    Args:\n",
        "        source_directory (str): Directory contains source documents\n",
        "        embeddings (Callable): Function to convert text to vector embeddings\n",
        "        output_directory (str, optional): Output directory of vector index database. Defaults to \"./vectorstore\".\n",
        "        emb_store_type (str, optional): Type of vector index database. Defaults to \"faiss\".\n",
        "        chunk_size (int, optional): Maximum size of text chunks (characters) after split. Defaults to 1000.\n",
        "        chunk_overlap (int, optional): Maximum overlapping window between text chunks. Defaults to 250.\n",
        "        exclude_pages (Optional[Dict], optional): Dictionary of pages to be excluded from documents. Defaults to None.\n",
        "        pinecone_idx_name (Optional[str], optional): Name of pinecone index to be created or loaded. Defaults to None.\n",
        "        additional_docs (Optional[str], optional): Additional Tables, Images or Json to be added to doc list. Defaults to None.\n",
        "        key_path (Optional[str], optional): Path to file containing API info.\n",
        "            Defaults to os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\").\n",
        "\n",
        "    Returns:\n",
        "        Vectorstore: Vector Database\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(output_directory):\n",
        "        rmtree(output_directory)\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    if source_directory:\n",
        "        LOGGER.info(f\"Loading documents from {source_directory}\")\n",
        "\n",
        "        documents = load_documents(source_directory, exclude_pages=exclude_pages)\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        LOGGER.info(f\"Loaded {len(documents)} documents from {source_directory}\")\n",
        "        LOGGER.info(\n",
        "            f\"Split into {len(texts)} chunks of text (max. {chunk_size} characters each)\"\n",
        "        )\n",
        "    else:\n",
        "        texts = []\n",
        "\n",
        "    if additional_docs:\n",
        "        texts.extend(additional_docs)\n",
        "\n",
        "    LOGGER.info(\n",
        "        f\"Total number of text chunks to create vector index store: {len(texts)}\"\n",
        "    )\n",
        "\n",
        "    if emb_store_type == \"chroma\":\n",
        "        chroma_settings = Settings(\n",
        "            chroma_db_impl=\"duckdb+parquet\",\n",
        "            persist_directory=output_directory,\n",
        "            anonymized_telemetry=False,\n",
        "        )\n",
        "        db = Chroma.from_documents(\n",
        "            texts,\n",
        "            embeddings,\n",
        "            persist_directory=output_directory,\n",
        "            client_settings=chroma_settings,\n",
        "        )\n",
        "        db.persist()\n",
        "\n",
        "    elif emb_store_type == \"faiss\":\n",
        "        db = FAISS.from_documents(texts, embedding=embeddings)\n",
        "        db.save_local(output_directory)\n",
        "        assert \"index.faiss\" in os.listdir(\n",
        "            output_directory\n",
        "        ) and \"index.pkl\" in os.listdir(output_directory)\n",
        "\n",
        "    elif emb_store_type == \"pinecone\":\n",
        "        with open(key_path, \"r\") as f:\n",
        "            keys = json.loads(f)\n",
        "        PINECONE_API_KEY = keys[\"PINECONE_API\"][\"KEY\"]\n",
        "        PINECONE_ENV = keys[\"PINECONE_API\"][\"ENV\"]\n",
        "\n",
        "        pinecone.init(\n",
        "            api_key=PINECONE_API_KEY,\n",
        "            environment=PINECONE_ENV,\n",
        "        )\n",
        "\n",
        "        if not pinecone_idx_name:\n",
        "            pinecone_idx_name = \"index_{}\".format(\n",
        "                datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
        "            )\n",
        "\n",
        "        if pinecone_idx_name not in pinecone.list_indexes():\n",
        "            db = Pinecone.from_documents(\n",
        "                texts, embedding=embeddings, index_name=pinecone_idx_name\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            db = Pinecone.from_existing_index(pinecone_idx_name, embeddings)\n",
        "            db.add_documents(texts)\n",
        "\n",
        "    LOGGER.info(\n",
        "        f\"Successfully created {emb_store_type} vectorstore at {output_directory}\"\n",
        "    )\n",
        "\n",
        "    return db\n",
        "    \n",
        "def check_documents_token(\n",
        "    docs: List[Document],\n",
        "    llm = ChatOpenAI(temperature=0,\n",
        "                     model_name=\"gpt-3.5-turbo\",\n",
        "                     openai_api_key=OPENAI_KEY)\n",
        "    ):\n",
        "    if not isinstance(docs, List):\n",
        "        docs = [docs]\n",
        "    combine_document_chain = StuffDocumentsChain(\n",
        "        llm_chain=LLMChain(\n",
        "            llm=llm,\n",
        "            prompt=PromptTemplate(template=\"{summaries}\",\n",
        "                                input_variables=[\"summaries\"]),\n",
        "            verbose=False,\n",
        "        ),\n",
        "        verbose=False\n",
        "    )\n",
        "    return combine_document_chain.prompt_length(docs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BP89lkLGsmKF"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MapReduceDocumentsChainV2(MapReduceDocumentsChain):\n",
        "    combine_max_tokens: int = 30000\n",
        "    collapse_max_tokens: int = 5000\n",
        "\n",
        "    @root_validator()\n",
        "    def check_maximum_context_length(cls, values: Dict) -> Dict:\n",
        "        max_token_dict = {\n",
        "            \"gpt-3.5-turbo\": 3000,\n",
        "            \"gpt-3.5-turbo-16k\": 14000,\n",
        "            \"gpt-4\": 7000,\n",
        "            \"gpt-4-32k\": 30000\n",
        "        }\n",
        "        \n",
        "        combine_doc_llm_model = values[\"combine_document_chain\"].llm_chain.llm.model_name\n",
        "        if combine_doc_llm_model in max_token_dict:\n",
        "            if max_token_dict[combine_doc_llm_model] < values[\"combine_max_tokens\"]:\n",
        "                values[\"combine_max_tokens\"] = max_token_dict[combine_doc_llm_model]\n",
        "        \n",
        "        if values[\"collapse_document_chain\"]:\n",
        "            collapse_doc_llm_model = values[\"collapse_document_chain\"].llm_chain.llm.model_name\n",
        "        else:\n",
        "            collapse_doc_llm_model = values[\"combine_document_chain\"].llm_chain.llm.model_name\n",
        "        \n",
        "        if collapse_doc_llm_model in max_token_dict:\n",
        "            if max_token_dict[collapse_doc_llm_model] < values[\"collapse_max_tokens\"]:\n",
        "                values[\"collapse_max_tokens\"] = max_token_dict[collapse_doc_llm_model]\n",
        "\n",
        "        return values\n",
        "\n",
        "    def combine_docs(\n",
        "        self,\n",
        "        docs: List[Document],\n",
        "        callbacks: Callbacks = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Tuple[str, dict]:\n",
        "        \"\"\"Combine documents in a map reduce manner.\n",
        "\n",
        "        Combine by mapping first chain over all documents, then reducing the results.\n",
        "        This reducing can be done recursively if needed (if there are many documents).\n",
        "        \"\"\"\n",
        "        results = self.llm_chain.apply(\n",
        "            # FYI - this is parallelized and so it is fast.\n",
        "            [{self.document_variable_name: d.page_content, **kwargs} for d in docs],\n",
        "            callbacks=callbacks,\n",
        "        )\n",
        "        return self._process_results(\n",
        "            results, docs, callbacks=callbacks, **kwargs\n",
        "        )\n",
        "\n",
        "    def _process_results(\n",
        "        self,\n",
        "        results: List[Dict],\n",
        "        docs: List[Document],\n",
        "        callbacks: Callbacks = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Tuple[str, dict]:\n",
        "        question_result_key = self.llm_chain.output_key\n",
        "        result_docs = [\n",
        "            Document(page_content=r[question_result_key], metadata=docs[i].metadata)\n",
        "            # This uses metadata from the docs, and the textual results from `results`\n",
        "            for i, r in enumerate(results)\n",
        "        ]\n",
        "        length_func = self.combine_document_chain.prompt_length\n",
        "        num_tokens = length_func(result_docs, **kwargs)\n",
        "\n",
        "        def _collapse_docs_func(docs: List[Document], **kwargs: Any) -> str:\n",
        "            return self._collapse_chain.run(\n",
        "                input_documents=docs, callbacks=callbacks, **kwargs\n",
        "            )\n",
        "\n",
        "        collapse_counter = 0\n",
        "        while num_tokens is not None and num_tokens > self.combine_max_tokens:\n",
        "            \n",
        "            # \n",
        "            collapse_counter += 1\n",
        "            if collapse_counter == 2:\n",
        "                raise Exception(\"Double Collapse steps. Stop\")            \n",
        "            \n",
        "            new_result_doc_list = _split_list_of_docs(\n",
        "                result_docs, length_func, self.collapse_max_tokens, **kwargs\n",
        "            )\n",
        "            result_docs = []\n",
        "            for docs in new_result_doc_list:\n",
        "                new_doc = _collapse_docs(docs, _collapse_docs_func, **kwargs)\n",
        "                result_docs.append(new_doc)\n",
        "            num_tokens = self.combine_document_chain.prompt_length(\n",
        "                result_docs, **kwargs\n",
        "            )\n",
        "        if self.return_intermediate_steps:\n",
        "            _results = [r[self.llm_chain.output_key] for r in results]\n",
        "            extra_return_dict = {\"intermediate_steps\": _results}\n",
        "        else:\n",
        "            extra_return_dict = {}\n",
        "        output = self.combine_document_chain.run(\n",
        "            input_documents=result_docs, callbacks=callbacks, **kwargs\n",
        "        )\n",
        "        return output, extra_return_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OfcdafLxXEQx"
      },
      "outputs": [],
      "source": [
        "class BaseExperiment:\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm_type: str = \"gpt-3.5-turbo\",\n",
        "        keys_json: str = osp.join(MAIN_DIR, \"auth\", \"api_keys.json\"),\n",
        "        temperature: float = 0,\n",
        "        max_tokens: int = 512,\n",
        "        gt: Optional[str] = None,\n",
        "        verbose: bool = False,\n",
        "    ):\n",
        "        self.llm_type = llm_type.lower()\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "        with open(keys_json, \"r\") as f:\n",
        "            keys = json.load(f)\n",
        "\n",
        "        self.openai_key = (\n",
        "            keys[\"OPENAI_API_KEY_FOR_GPT4\"]\n",
        "            if self.llm_type == \"gpt-4\"\n",
        "            else keys[\"OPENAI_API_KEY\"]\n",
        "        )\n",
        "        \n",
        "        self.ground_truth = self.load_groundtruth(gt) if gt else None    \n",
        "        self.chain = None\n",
        "        self.verbose = verbose\n",
        "        \n",
        "    @abstractmethod\n",
        "    def run_test_cases(\n",
        "        self, test_cases: Union[List[str], str], **kwargs\n",
        "    ):\n",
        "        return NotImplementedError\n",
        "    \n",
        "    @staticmethod\n",
        "    def convert_prompt_to_string(\n",
        "        prompt: Union[PromptTemplate, ChatPromptTemplate]\n",
        "    ) -> str:\n",
        "        \"\"\"Convert Prompt Object to string format\n",
        "\n",
        "        Args:\n",
        "            prompt (Union[PromptTemplate, ChatPromptTemplate]): Prompt Template\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt String Template\n",
        "        \"\"\"\n",
        "        return prompt.format(**{v: v for v in prompt.input_variables})\n",
        "    \n",
        "    def load_groundtruth(self, gt_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load Ground Truth information from .csv file\n",
        "\n",
        "        Args:\n",
        "            gt_path (str): Path to Ground Truth file\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame containing Ground Truth data.\n",
        "        \"\"\"\n",
        "        return pd.read_csv(gt_path, encoding=\"ISO-8859-1\")\n",
        "\n",
        "class QuestionAnsweringWithIndexSearchExperiment(BaseExperiment):\n",
        "    \"\"\"Experiment Module\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        prompt_template: Union[PromptTemplate, ChatPromptTemplate],\n",
        "        vector_store: str,\n",
        "        llm_type: str = \"gpt-3.5-turbo\",\n",
        "        emb: str = \"text-embedding-ada-002\",\n",
        "        keys_json: str = osp.join(MAIN_DIR, \"auth\", \"api_keys.json\"),\n",
        "        temperature: float = 0,\n",
        "        max_tokens: int = 512,\n",
        "        gt: Optional[str] = None,\n",
        "        verbose: bool = False,\n",
        "        k: int = 4,\n",
        "        max_tokens_limit: int = 3375,\n",
        "        reduce_k_below_max_tokens: bool = True,\n",
        "    ):\n",
        "        \"\"\"Initiate Instance for an experiment run\n",
        "\n",
        "        Args:\n",
        "            prompt_template (Union[PromptTemplate, ChatPromptTemplate]): Prompt to be feed to LLM\n",
        "            vector_store (str): Path to Vector Index Database\n",
        "            llm_type (str, optional): Type of LLM Model. Defaults to \"gpt-3.5-turbo\".\n",
        "            emb (str, optional): Type of Embedding Model. Defaults to \"text-embedding-ada-002\".\n",
        "            keys_json (str, optional): Path to API Keys. Defaults to osp.join(MAIN_DIR, \"auth\", \"api_keys.json\").\n",
        "            temperature (float, optional): Temperature Settings for LLM model. Lower temperature makes LLM more deterministic\n",
        "                while higher temperature makes LLM more random. Defaults to 0.\n",
        "            max_tokens (int, optional): Max_Tokens Settings for LLM model. Defaults to 512.\n",
        "            gt (Optional[str], optional): Path to Ground Truth file. Defaults to None.\n",
        "            verbose (bool, optional): Verbose Setting. Defaults to False.\n",
        "        \"\"\"\n",
        "\n",
        "        super(QuestionAnsweringWithIndexSearchExperiment, self).__init__(\n",
        "            llm_type=llm_type,\n",
        "            keys_json=keys_json,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            gt=gt,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        \n",
        "        if isinstance(prompt_template, ChatPromptTemplate):\n",
        "            self.llm = ChatOpenAI(\n",
        "                model_name=self.llm_type,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=self.max_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "        else:\n",
        "            self.llm = OpenAI(\n",
        "                model_name=self.llm_type,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=self.max_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "            \n",
        "        self.embedder = OpenAIEmbeddings(model=emb, openai_api_key=self.openai_key)\n",
        "        try:\n",
        "            self.load_vectorstore(vector_store)\n",
        "        except Exception:\n",
        "            print(\n",
        "                \"Vectorstore invalid. Please load valid vectorstore or create new vectorstore.\"\n",
        "            )\n",
        "\n",
        "        self.k = k\n",
        "        self.max_tokens_limit = max_tokens_limit\n",
        "        self.reduce_k_below_max_tokens = reduce_k_below_max_tokens\n",
        "        self.prompt_template = prompt_template\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        self.sources = []\n",
        "        self.drug_parser = OutputFixingParser.from_llm(parser= PydanticOutputParser(pydantic_object=DrugOutput),\n",
        "                                                       llm=ChatOpenAI(model_name=\"gpt-4\",\n",
        "                                                                      temperature=0,\n",
        "                                                                      openai_api_key=OPENAI_KEY)\n",
        "                                                       )\n",
        "       \n",
        "\n",
        "    def load_vectorstore(self, vectorstore_path: str):\n",
        "        \"\"\"Load Vectorstore from path\n",
        "\n",
        "        Args:\n",
        "            vectorstore_path (str): Path to vector database folder.\n",
        "        \"\"\"\n",
        "        assert \"index.faiss\" in os.listdir(\n",
        "            vectorstore_path\n",
        "        ) and \"index.pkl\" in os.listdir(vectorstore_path), \"Invalid Vectorstore\"\n",
        "        self.docsearch = FAISS.load_local(vectorstore_path, self.embedder)\n",
        "        LOGGER.info(\"Successfully loaded existing vectorstore from local storage\")\n",
        "\n",
        "    def generate_vectorstore(\n",
        "        self,\n",
        "        data_directory: Optional[str] = None,\n",
        "        output_directory: str = \"./vectorstore\",\n",
        "        emb_store_type: str = \"faiss\",\n",
        "        chunk_size: int = 1000,\n",
        "        chunk_overlap: int = 250,\n",
        "        exclude_pages: Optional[Dict] = None,\n",
        "        pinecone_idx_name: Optional[str] = None,\n",
        "        additional_docs: Optional[str] = None,\n",
        "        key_path: Optional[str] = os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"),\n",
        "    ):\n",
        "        \"\"\"Generate New vectorstore\n",
        "\n",
        "        Args:\n",
        "            data_directory (str): Directory contains source documents\n",
        "            output_directory (str, optional): Output directory of vector index database. Defaults to \"./vectorstore\".\n",
        "            emb_store_type (str, optional): Type of vector index database. Defaults to \"faiss\".\n",
        "            chunk_size (int, optional): Maximum size of text chunks (characters) after split. Defaults to 1000.\n",
        "            chunk_overlap (int, optional): Maximum overlapping window between text chunks. Defaults to 250.\n",
        "            exclude_pages (Optional[Dict], optional): Dictionary of pages to be excluded from documents. Defaults to None.\n",
        "            pinecone_idx_name (Optional[str], optional): Name of pinecone index to be created or loaded. Defaults to None.\n",
        "            additional_docs (Optional[str], optional): Additional Tables, Images or Json to be added to doc list. Defaults to None.\n",
        "            key_path (Optional[str], optional): Path to file containing API info.\n",
        "                Defaults to os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\").\n",
        "        \"\"\"\n",
        "        self.docsearch = generate_vectorstore(\n",
        "            data_directory=data_directory,\n",
        "            embedder=self.embedder,\n",
        "            output_directory=output_directory,\n",
        "            emb_store_type=emb_store_type,\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            exclude_pages=exclude_pages,\n",
        "            pinecone_idx_name=pinecone_idx_name,\n",
        "            additional_docs=additional_docs,\n",
        "            key_path=key_path,\n",
        "        )\n",
        "\n",
        "    def run_test_cases(\n",
        "        self,\n",
        "        test_cases: Union[List[str], str],\n",
        "        only_return_source: bool = False,\n",
        "        chain_type: Literal[\"stuff\", \"refine\", \"map_reduce\", \"map_rerank\"] = \"stuff\"\n",
        "    ):\n",
        "        \"\"\"Run and save test cases to memory\n",
        "\n",
        "        Args:\n",
        "            test_cases (Union[List[str], str]): List of test queries.\n",
        "        \"\"\"\n",
        "        if isinstance(test_cases, str):\n",
        "            with open(test_cases, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "                test_cases = f.readlines()\n",
        "            test_cases = [test_case.rstrip() for test_case in test_cases]\n",
        "\n",
        "        if not self.chain:\n",
        "            self._create_chain(chain_type = chain_type)\n",
        "\n",
        "        if only_return_source:\n",
        "            LOGGER.info(\"Perform Semantic Search for Source Documents only (No QA).\")\n",
        "\n",
        "        for test_case in test_cases:\n",
        "            print(\"Query: {}\".format(test_case))\n",
        "            sources = []  # All sources for 1 single query\n",
        "            if only_return_source:\n",
        "                self.questions.append(test_case)\n",
        "                self.answers.append(None)\n",
        "                inputs = {\"question\": test_case}\n",
        "                source_documents = self.chain._get_docs(inputs)\n",
        "\n",
        "            else:\n",
        "                output = self.chain(test_case)\n",
        "                self.questions.append(output[\"question\"])\n",
        "                self.answers.append(output[\"answer\"])\n",
        "                source_documents = output[\"source_documents\"]\n",
        "\n",
        "            for document in source_documents:\n",
        "                sources.append(\n",
        "                    {\n",
        "                        \"title\": document.metadata[\"title\"],\n",
        "                        \"filename\": document.metadata[\"source\"].split(\"/\")[-1],\n",
        "                        \"page\": document.metadata[\"page\"],\n",
        "                        \"text\": document.page_content,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            self.sources.append(sources)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset queries and answers\"\"\"\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        self.sources = []\n",
        "        self.ground_truth = None\n",
        "\n",
        "    @staticmethod\n",
        "    def process_source(source: Dict) -> str:\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            source (Dict): Source Document Information\n",
        "\n",
        "        Returns:\n",
        "            str: Source Document Information in string\n",
        "        \"\"\"\n",
        "        return \"\\n\\n\".join([f\"{k}: {v}\" for k, v in source.items()])\n",
        "\n",
        "    def save_json(self, output_path: str):\n",
        "        \"\"\"Save Output of test case runs to json file\n",
        "\n",
        "        Args:\n",
        "            output_path (str): Output Path to json file.\n",
        "        \"\"\"\n",
        "        output_dict = {}\n",
        "        output_dict[\"prompt\"] = QuestionAnsweringWithIndexSearchExperiment.convert_prompt_to_string(\n",
        "            self.prompt_template\n",
        "        )\n",
        "        output_dict[\"test_cases\"] = []\n",
        "\n",
        "        for question, answer, source in zip(self.questions, self.answers, self.sources):\n",
        "            output_dict[\"test_cases\"].append(\n",
        "                {\"question\": question, \"answer\": answer, \"sources\": source}\n",
        "            )\n",
        "\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(output_dict, f)\n",
        "\n",
        "    def load_json(self, json_path: str, reset: bool = False):\n",
        "        \"\"\"Load Queries and Answers from Json file\n",
        "\n",
        "        Args:\n",
        "            json_path (str): Path to json output file to load into instance\n",
        "            reset (bool, optional): If reset, clear queries and answers from memory before loading. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.reset()\n",
        "        with open(json_path, \"r\") as f:\n",
        "            input_dict = json.load(f)\n",
        "        for test_case in input_dict[\"test_cases\"]:\n",
        "            self.questions.append(test_case[\"question\"])\n",
        "            self.answers.append(test_case[\"answer\"])\n",
        "            self.sources.append(test_case[\"sources\"])\n",
        "\n",
        "    def write_csv(self, output_csv: str):\n",
        "        \"\"\"Write questions and answers to .csv files\n",
        "\n",
        "        Args:\n",
        "            output_csv (str): Path to output csv file\n",
        "        \"\"\"\n",
        "\n",
        "        pd_answers = [[], []]\n",
        "        pd_pros = [[], []]\n",
        "        pd_cons = [[], []]\n",
        "        pd_sources = [[], [], [], [], [], []]\n",
        "\n",
        "        for answer, sources in zip(self.answers, self.sources):\n",
        "            if answer:\n",
        "                drugs_info = re.findall(re.compile(r\"{[^{}]+}\"), answer)\n",
        "                drugs = []\n",
        "                for drug in drugs_info:\n",
        "                    try:\n",
        "                        drug = self.drug_parser.parse(drug)\n",
        "                        drugs.append(drug)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "            else:\n",
        "                drugs = []\n",
        "                \n",
        "            pd_answers[0].append(drugs[0].drug_name if len(drugs) > 0 else None)\n",
        "            pd_answers[1].append(drugs[1].drug_name if len(drugs) > 1 else None)\n",
        "            pd_pros[0].append(drugs[0].advantages if len(drugs) > 0 else None)\n",
        "            pd_cons[0].append(drugs[0].disadvantages if len(drugs) > 0 else None)\n",
        "            pd_pros[1].append(drugs[1].advantages if len(drugs) > 1 else None)\n",
        "            pd_cons[1].append(drugs[1].disadvantages if len(drugs) > 1 else None)\n",
        "\n",
        "            for idx, source in enumerate(sources):\n",
        "                pd_sources[idx].append(QuestionAnsweringWithIndexSearchExperiment.process_source(source))\n",
        "\n",
        "            if idx + 1 < len(pd_sources):\n",
        "                for i in range(idx + 1, len(pd_sources)):\n",
        "                    pd_sources[i].append(None)\n",
        "\n",
        "        info = {\"question\": self.questions}\n",
        "\n",
        "        if self.ground_truth is not None:\n",
        "            info[\"gt_rec1\"] = self.ground_truth[\"Recommendation 1\"].tolist()\n",
        "            info[\"gt_rec2\"] = self.ground_truth[\"Recommendation 2\"].tolist()\n",
        "            info[\"gt_rec3\"] = self.ground_truth[\"Recommendation 3\"].tolist()\n",
        "            info[\"gt_avoid\"] = self.ground_truth[\"Drug Avoid\"].tolist()\n",
        "            info[\"gt_reason\"] = self.ground_truth[\"Reasoning\"].tolist()\n",
        "\n",
        "        info[\"prompt\"] = [\n",
        "            QuestionAnsweringWithIndexSearchExperiment.convert_prompt_to_string(self.prompt_template)\n",
        "        ] * len(self.questions)\n",
        "        info[\"raw_answer\"] = self.answers\n",
        "        info[\"answer1\"] = pd_answers[0]\n",
        "        info[\"pro1\"] = pd_pros[0]\n",
        "        info[\"cons1\"] = pd_cons[0]\n",
        "        info[\"answer2\"] = pd_answers[1]\n",
        "        info[\"pro2\"] = pd_pros[1]\n",
        "        info[\"cons2\"] = pd_cons[1]\n",
        "        info[\"source1\"] = pd_sources[0]\n",
        "        info[\"source2\"] = pd_sources[1]\n",
        "        info[\"source3\"] = pd_sources[2]\n",
        "        info[\"source4\"] = pd_sources[3]\n",
        "        info[\"source5\"] = pd_sources[4]\n",
        "        info[\"source6\"] = pd_sources[5]\n",
        "\n",
        "        panda_df = pd.DataFrame(info)\n",
        "\n",
        "        panda_df.to_csv(output_csv, header=True)\n",
        "\n",
        "    def _create_chain(\n",
        "        self,\n",
        "        chain_type: str = \"stuff\",\n",
        "        return_source_documents: bool = True,\n",
        "    ):\n",
        "        \"\"\"Initiate QA from Source Chain\n",
        "\n",
        "        Args:\n",
        "            chain_type (str, optional): Chain Type. Can be stuff|map_reduce|refine|map_rerank. Defaults to \"stuff\".\n",
        "            return_source_documents (bool, optional): Whether to return source documents along side answers. Defaults to True.\n",
        "        \"\"\"\n",
        "        self.chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=chain_type,\n",
        "            retriever=self.docsearch.as_retriever(k = self.k),\n",
        "            return_source_documents=return_source_documents,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt_template},\n",
        "            max_tokens_limit=self.max_tokens_limit,\n",
        "            reduce_k_below_max_tokens=self.reduce_k_below_max_tokens,\n",
        "            verbose=self.verbose,\n",
        "        )\n",
        "    \n",
        "class QuestionAnsweringOverDocsExperiment(BaseExperiment):\n",
        "    \"\"\"QAOverDocs Base Experiment Module\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys_json: str = osp.join(MAIN_DIR, \"auth\", \"api_keys.json\"),\n",
        "        temperature: float = 0,\n",
        "        gt: Optional[str] = None,\n",
        "        verbose: bool = False,\n",
        "    ):\n",
        "        \"\"\"QAOverDocs Base Experiment Module\n",
        "\n",
        "        Args:\n",
        "            keys_json (str, optional): Path to API Keys. Defaults to osp.join(MAIN_DIR, \"auth\", \"api_keys.json\").\n",
        "            temperature (float, optional): Temperature Settings for LLM model. Defaults to 0.\n",
        "            gt (Optional[str], optional): Path to Ground Truth file. Defaults to None.\n",
        "            verbose (bool, optional): Verbose Setting. Defaults to False.\n",
        "        \"\"\"\n",
        "        super(QuestionAnsweringOverDocsExperiment, self).__init__(\n",
        "            keys_json=keys_json,\n",
        "            temperature=temperature,\n",
        "            gt=gt,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        self.intermediate_steps = []\n",
        "        self.prompt_map = {}\n",
        "\n",
        "        self.drug_parser = DrugParser.from_llm(\n",
        "            parser=PydanticOutputParser(pydantic_object=DrugOutput),\n",
        "            llm=ChatOpenAI(\n",
        "                model_name=\"gpt-4\", temperature=0, openai_api_key=self.openai_key\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def run_test_cases(\n",
        "        self,\n",
        "        test_cases: Union[List[str], str],\n",
        "        docs: List[Document],\n",
        "        return_intermediate_steps: bool = False,\n",
        "    ):\n",
        "        \"\"\"Run and save test cases to memory\n",
        "\n",
        "        Args:\n",
        "            test_cases (Union[List[str], str]): List of test queries.\n",
        "            docs (List[Document]): List of input documents\n",
        "            return_intermediate_steps (bool, optional): Return intermediate steps. Defaults to False.\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(test_cases, str):\n",
        "            with open(test_cases, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "                test_cases = f.readlines()\n",
        "            test_cases = [test_case.rstrip() for test_case in test_cases]\n",
        "\n",
        "        if not self.chain:\n",
        "            self._create_chain(return_intermediate_steps=return_intermediate_steps)\n",
        "\n",
        "        if self.chain._chain_type == \"map_reduce_documents_chain\":\n",
        "            no_tokens = sum(\n",
        "                [\n",
        "                    self.chain.combine_document_chain.llm_chain.llm.get_num_tokens(\n",
        "                        doc.page_content\n",
        "                    )\n",
        "                    for doc in docs\n",
        "                ]\n",
        "            )\n",
        "            LOGGER.info(f\"Number of tokens from all documents: {no_tokens}\")\n",
        "\n",
        "        for test_case in test_cases:\n",
        "            print(\"Query: {}\".format(test_case))\n",
        "            output = self.chain({\"input_documents\": docs, \"question\": test_case})\n",
        "            self.questions.append(output[\"question\"])\n",
        "            self.answers.append(output[\"output_text\"])\n",
        "            self.intermediate_steps.append(\n",
        "                output[\"intermediate_steps\"] if return_intermediate_steps else None\n",
        "            )\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset queries and answers\"\"\"\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        self.intermediate_steps = []\n",
        "\n",
        "    @abstractmethod\n",
        "    def _create_chain(\n",
        "        self, return_intermediate_steps: bool = False\n",
        "    ) -> BaseCombineDocumentsChain:\n",
        "        \"\"\"Initiate Main Chain for QA\n",
        "\n",
        "        Args:\n",
        "            return_intermediate_steps (bool, optional): Return Intermediate Steps. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            BaseCombineDocumentsChain: QA Chain\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def save_json(self, output_path: str):\n",
        "        \"\"\"Save Output of test case runs to json file\n",
        "\n",
        "        Args:\n",
        "            output_path (str): Output Path to json file.\n",
        "        \"\"\"\n",
        "        output_dict = {\n",
        "            \"prompt\": {\n",
        "                prompt_type: BaseExperiment.convert_prompt_to_string(prompt)\n",
        "                for prompt_type, prompt in self.prompt_map.items()\n",
        "            },\n",
        "            \"test_cases\": [],\n",
        "        }\n",
        "\n",
        "        for question, answer, intermediate_steps in zip(\n",
        "            self.questions, self.answers, self.intermediate_steps\n",
        "        ):\n",
        "            output_dict[\"test_cases\"].append(\n",
        "                {\n",
        "                    \"question\": question,\n",
        "                    \"answer\": answer,\n",
        "                    \"intermediate_steps\": intermediate_steps,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(output_dict, f)\n",
        "\n",
        "    def load_json(self, json_path: str, reset: bool = False):\n",
        "        \"\"\"Load Queries and Answers from Json file\n",
        "\n",
        "        Args:\n",
        "            json_path (str): Path to json output file to load into instance\n",
        "            reset (bool, optional): If reset, clear queries and answers from memory before loading. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.reset()\n",
        "        with open(json_path, \"r\") as f:\n",
        "            input_dict = json.load(f)\n",
        "        for test_case in input_dict[\"test_cases\"]:\n",
        "            self.questions.append(test_case[\"question\"])\n",
        "            self.answers.append(test_case[\"answer\"])\n",
        "            self.intermediate_steps.append(test_case[\"intermediate_steps\"])\n",
        "        LOGGER.info(\"Json file loaded successfully into Experiment instance.\")\n",
        "\n",
        "    def write_csv(self, output_csv: str):\n",
        "        \"\"\"Write Output to csv file\n",
        "\n",
        "        Args:\n",
        "            output_csv (str): Path to csv file\n",
        "        \"\"\"\n",
        "        info = {\"question\": self.questions}\n",
        "\n",
        "        if self.ground_truth is not None:\n",
        "            info[\"gt_rec1\"] = self.ground_truth[\"Recommendation 1\"].tolist()\n",
        "            info[\"gt_rec2\"] = self.ground_truth[\"Recommendation 2\"].tolist()\n",
        "            info[\"gt_rec3\"] = self.ground_truth[\"Recommendation 3\"].tolist()\n",
        "            info[\"gt_avoid\"] = self.ground_truth[\"Drug Avoid\"].tolist()\n",
        "            info[\"gt_reason\"] = self.ground_truth[\"Reasoning\"].tolist()\n",
        "\n",
        "        for prompt_type, prompt in self.prompt_map.items():\n",
        "            info[f\"{prompt_type}_prompt\"] = [\n",
        "                BaseExperiment.convert_prompt_to_string(prompt)\n",
        "            ] * len(self.questions)\n",
        "\n",
        "        pd_answers = [[], []]\n",
        "        pd_pros = [[], []]\n",
        "        pd_cons = [[], []]\n",
        "        for answer in self.answers:\n",
        "            if answer:\n",
        "                try:\n",
        "                    drugs = self.drug_parser.parse(answer)\n",
        "                except Exception:\n",
        "                    raise Exception(\"Cannot parse answer properly.\")\n",
        "            else:\n",
        "                drugs = []\n",
        "\n",
        "            pd_answers[0].append(drugs[0].drug_name if len(drugs) > 0 else None)\n",
        "            pd_answers[1].append(drugs[1].drug_name if len(drugs) > 1 else None)\n",
        "            pd_pros[0].append(drugs[0].advantages if len(drugs) > 0 else None)\n",
        "            pd_cons[0].append(drugs[0].disadvantages if len(drugs) > 0 else None)\n",
        "            pd_pros[1].append(drugs[1].advantages if len(drugs) > 1 else None)\n",
        "            pd_cons[1].append(drugs[1].disadvantages if len(drugs) > 1 else None)\n",
        "\n",
        "        info[\"raw_answer\"] = self.answers\n",
        "        info[\"answer1\"] = pd_answers[0]\n",
        "        info[\"pro1\"] = pd_pros[0]\n",
        "        info[\"cons1\"] = pd_cons[0]\n",
        "        info[\"answer2\"] = pd_answers[1]\n",
        "        info[\"pro2\"] = pd_pros[1]\n",
        "        info[\"cons2\"] = pd_cons[1]\n",
        "\n",
        "        panda_df = pd.DataFrame(info)\n",
        "\n",
        "        panda_df.to_csv(output_csv, header=True)\n",
        "\n",
        "\n",
        "class MapReduceQAOverDocsExperiment(QuestionAnsweringOverDocsExperiment):\n",
        "    \"\"\"MapReduce QA Experiment\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        map_prompt: Optional[Union[PromptTemplate, ChatPromptTemplate]],\n",
        "        combine_prompt: Optional[Union[PromptTemplate, ChatPromptTemplate]],\n",
        "        collapse_prompt: Optional[Union[PromptTemplate, ChatPromptTemplate]] = None,\n",
        "        llm_type: str = \"gpt-3.5-turbo\",\n",
        "        reduce_llm: Optional[str] = None,\n",
        "        collapse_llm: Optional[str] = None,\n",
        "        keys_json: str = osp.join(MAIN_DIR, \"auth\", \"api_keys.json\"),\n",
        "        temperature: float = 0,\n",
        "        max_gen_tokens: int = 512,\n",
        "        combine_max_gen_tokens: int = 512,\n",
        "        collapse_max_gen_tokens: int = 512,\n",
        "        combine_max_doc_tokens: int = 14000,\n",
        "        collapse_max_doc_tokens: int = 6000,\n",
        "        gt: Optional[str] = None,\n",
        "        verbose: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"MapReduce QA Experiment\n",
        "\n",
        "        Args:\n",
        "            map_prompt (Optional[Union[PromptTemplate, ChatPromptTemplate]]): Prompt to perform QA on each document\n",
        "            combine_prompt (Optional[Union[PromptTemplate, ChatPromptTemplate]]): Prompt to combine all output for final answer\n",
        "            collapse_prompt (Optional[Union[PromptTemplate, ChatPromptTemplate]], optional): Prompt to answer on small groups of documents. Defaults to None.\n",
        "            llm_type (str, optional): Base LLM. By default use for map chain. Defaults to \"gpt-3.5-turbo\".\n",
        "            reduce_llm (Optional[str], optional): LLM use for final combine step. Defaults to None.\n",
        "            collapse_llm (Optional[str], optional): LLM use for collapse step. Defaults to None.\n",
        "            keys_json (str, optional): Path to API keys. Defaults to osp.join(MAIN_DIR, \"auth\", \"api_keys.json\").\n",
        "            temperature (float, optional): Temperature of all LLM models. Defaults to 0.\n",
        "            max_gen_tokens (int, optional): Max allowed tokens generated for base LLM. Defaults to 512.\n",
        "            combine_max_gen_tokens (int, optional): Max allowed tokens generated for combine LLM.. Defaults to 512.\n",
        "            collapse_max_gen_tokens (int, optional): Max allowed tokens generated for collapse LLM.. Defaults to 512.\n",
        "            combine_max_doc_tokens (int, optional): Maximum combined tokens at final reduce step. Defaults to 14000.\n",
        "            collapse_max_doc_tokens (int, optional): Maximum number of tokens allowed for each collapse step. Defaults to 6000.\n",
        "            gt (Optional[str], optional): Path to ground_truth file. Defaults to None.\n",
        "            verbose (bool, optional): Verbose settings. Defaults to False.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MapReduceQAOverDocsExperiment, self).__init__(\n",
        "            keys_json=keys_json,\n",
        "            temperature=temperature,\n",
        "            gt=gt,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "\n",
        "        if isinstance(map_prompt, ChatPromptTemplate):\n",
        "            self.llm = ChatOpenAI(\n",
        "                model_name=llm_type,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "        elif isinstance(map_prompt, PromptTemplate):\n",
        "            self.llm = OpenAI(\n",
        "                model_name=llm_type,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Incorrect Type of Map Prompt Template\")\n",
        "\n",
        "        reduce_llm = reduce_llm or llm_type\n",
        "        collapse_llm = collapse_llm or reduce_llm\n",
        "\n",
        "        if isinstance(combine_prompt, ChatPromptTemplate):\n",
        "            self.reduce_llm = ChatOpenAI(\n",
        "                model_name=reduce_llm,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=combine_max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "\n",
        "        elif isinstance(combine_prompt, PromptTemplate):\n",
        "            self.reduce_llm = OpenAI(\n",
        "                model_name=reduce_llm,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=combine_max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Incorrect Type of Combine Prompt Template\")\n",
        "\n",
        "        self.map_prompt = map_prompt\n",
        "        self.combine_prompt = combine_prompt\n",
        "        self.collapse_prompt = collapse_prompt or combine_prompt\n",
        "\n",
        "        self.prompt_map = {\n",
        "            \"map\": map_prompt,\n",
        "            \"combine\": combine_prompt,\n",
        "            \"collapse\": collapse_prompt,\n",
        "        }\n",
        "\n",
        "        if isinstance(self.collapse_prompt, ChatPromptTemplate):\n",
        "            self.collapse_llm = ChatOpenAI(\n",
        "                model_name=collapse_llm,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=collapse_max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "\n",
        "        elif isinstance(self.collapse_prompt, PromptTemplate):\n",
        "            self.collapse_llm = OpenAI(\n",
        "                model_name=collapse_llm,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=collapse_max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Incorrect Type of Collapse Prompt Template\")\n",
        "\n",
        "        self.combine_max_doc_tokens = combine_max_doc_tokens\n",
        "        self.collapse_max_doc_tokens = collapse_max_doc_tokens\n",
        "\n",
        "    def _create_chain(self, return_intermediate_steps: bool = False):\n",
        "        \"\"\"Initiate QA from Source Chain\n",
        "\n",
        "        Args:\n",
        "            return_intermediate_steps (bool, optional): Whether to return intermediate_steps. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        map_chain = LLMChain(llm=self.llm, prompt=self.map_prompt, verbose=self.verbose)\n",
        "\n",
        "        reduce_chain = LLMChain(\n",
        "            llm=self.reduce_llm, prompt=self.combine_prompt, verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        combine_document_chain = StuffDocumentsChain(\n",
        "            llm_chain=reduce_chain,\n",
        "            document_variable_name=\"summaries\",\n",
        "            verbose=self.verbose,\n",
        "        )\n",
        "\n",
        "        collapse_chain = LLMChain(\n",
        "            llm=self.collapse_llm, prompt=self.collapse_prompt, verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        collapse_document_chain = StuffDocumentsChain(\n",
        "            llm_chain=collapse_chain,\n",
        "            document_variable_name=\"summaries\",\n",
        "            verbose=self.verbose,\n",
        "        )\n",
        "\n",
        "        self.chain = MapReduceDocumentsChainV2(\n",
        "            llm_chain=map_chain,\n",
        "            combine_document_chain=combine_document_chain,\n",
        "            collapse_document_chain=collapse_document_chain,\n",
        "            document_variable_name=\"context\",\n",
        "            combine_max_tokens=self.combine_max_doc_tokens,\n",
        "            collapse_max_tokens=self.collapse_max_doc_tokens,\n",
        "            return_intermediate_steps=return_intermediate_steps,\n",
        "            return_map_steps=return_intermediate_steps,\n",
        "            verbose=self.verbose,\n",
        "        )\n",
        "\n",
        "\n",
        "class RefineQAOverDocsExperiment(QuestionAnsweringOverDocsExperiment):\n",
        "    \"\"\"Refine QA Experiment\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_prompt: Optional[Union[PromptTemplate, ChatPromptTemplate]],\n",
        "        refine_prompt: Optional[Union[PromptTemplate, ChatPromptTemplate]],\n",
        "        llm_type: str = \"gpt-3.5-turbo\",\n",
        "        refine_llm: Optional[str] = None,\n",
        "        keys_json: str = osp.join(MAIN_DIR, \"auth\", \"api_keys.json\"),\n",
        "        temperature: float = 0,\n",
        "        max_gen_tokens: int = 1024,\n",
        "        gt: Optional[str] = None,\n",
        "        verbose: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Refine QA Experiment\n",
        "\n",
        "        Args:\n",
        "            initial_prompt (Optional[Union[PromptTemplate, ChatPromptTemplate]]): Prompt for initial answer\n",
        "            refine_prompt (Optional[Union[PromptTemplate, ChatPromptTemplate]]): Prompt for each refine step.\n",
        "            llm_type (str, optional): Base LLM. By default use for initial & refine chain. Defaults to \"gpt-3.5-turbo\".\n",
        "            refine_llm (Optional[str], optional): LLM for refine chain. In None, assign to base llm type. Defaults to None.\n",
        "            keys_json (str, optional): Path to API keys file. Defaults to osp.join(MAIN_DIR, \"auth\", \"api_keys.json\").\n",
        "            temperature (float, optional): Temperature of all LLM models. Defaults to 0.\n",
        "            max_gen_tokens (int, optional): Maximum generated tokens allowed. Defaults to 1024.\n",
        "            gt (Optional[str], optional): Path to ground truth file. Defaults to None.\n",
        "            verbose (bool, optional): Verbose settings. Defaults to False.\n",
        "        \"\"\"\n",
        "\n",
        "        super(RefineQAOverDocsExperiment, self).__init__(\n",
        "            keys_json=keys_json,\n",
        "            temperature=temperature,\n",
        "            gt=gt,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "\n",
        "        if isinstance(initial_prompt, ChatPromptTemplate):\n",
        "            self.llm = ChatOpenAI(\n",
        "                model_name=llm_type,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "        elif isinstance(initial_prompt, PromptTemplate):\n",
        "            self.llm = OpenAI(\n",
        "                model_name=llm_type,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Incorrect Type of Map Prompt Template\")\n",
        "\n",
        "        refine_llm = refine_llm or llm_type\n",
        "\n",
        "        if isinstance(refine_prompt, ChatPromptTemplate):\n",
        "            self.refine_llm = ChatOpenAI(\n",
        "                model_name=refine_llm,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "\n",
        "        elif isinstance(refine_prompt, PromptTemplate):\n",
        "            self.refine_llm = OpenAI(\n",
        "                model_name=refine_llm,\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=max_gen_tokens,\n",
        "                openai_api_key=self.openai_key,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Incorrect Type of Combine Prompt Template\")\n",
        "\n",
        "        self.initial_prompt = initial_prompt\n",
        "        self.refine_prompt = refine_prompt\n",
        "\n",
        "        self.prompt_map = {\n",
        "            \"initial\": initial_prompt,\n",
        "            \"refine\": refine_prompt,\n",
        "        }\n",
        "\n",
        "    def _create_chain(self, return_intermediate_steps: bool = False):\n",
        "        \"\"\"Initiate QA from Source Chain\n",
        "\n",
        "        Args:\n",
        "            return_intermediate_steps (bool, optional): Whether to return intermediate_steps. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        initial_chain = LLMChain(\n",
        "            llm=self.llm, prompt=self.initial_prompt, verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        refine_chain = LLMChain(\n",
        "            llm=self.refine_llm, prompt=self.refine_prompt, verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        self.chain = RefineDocumentsChain(\n",
        "            initial_llm_chain=initial_chain,\n",
        "            refine_llm_chain=refine_chain,\n",
        "            document_variable_name=\"context_str\",\n",
        "            initial_response_name=\"existing_answer\",\n",
        "            return_intermediate_steps=return_intermediate_steps,\n",
        "            verbose=self.verbose,\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CBLfxKVohLSS"
      },
      "source": [
        "# Create Vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datastore_paths = [os.path.join(DOCUMENT_SOURCE, PROJECT, file_name) for file_name in os.listdir(os.path.join(DOCUMENT_SOURCE, PROJECT)) if file_name.endswith(\".pdf\")]\n",
        "print(\"Number of documents in datastore:\", len(datastore_paths))\n",
        "for i, path in enumerate(datastore_paths):\n",
        "    print(f\"Index {i + 1}: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_path = datastore_paths[0]\n",
        "\n",
        "sample_data = load_single_document(sample_path)\n",
        "print(\"Number of pages:\", len(sample_data))\n",
        "sample_page = sample_data[4]\n",
        "print(str(sample_page.metadata) + \"\\n\")\n",
        "content = sample_page.page_content\n",
        "print(f\"Text Length: {len(content)}\\n\")\n",
        "# content = re.sub(r\"\\t+\", \" \", content)\n",
        "pprint(content[:1000])\n",
        "metadata = sample_page.metadata\n",
        "pprint(metadata)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ada-Text-Embeddings-2: Text + Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "add_docs_path = os.path.join(MAIN_DIR, \"data/additional_docs.json\")\n",
        "\n",
        "with open(add_docs_path, \"r\") as f:\n",
        "    additional_documents = json.load(f)\n",
        "    \n",
        "add_docs = []\n",
        "for table_info in additional_documents:\n",
        "    add_docs.extend(convert_csv_to_documents(table_info))\n",
        "    \n",
        "add_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_model = OpenAIEmbeddings(openai_api_key = OPENAI_KEY)\n",
        "database_path = os.path.join(EMBSTORE_DIR, PROJECT,\n",
        "                                \"faiss\", \"text-embedding-ada-002\", \"v2-add\")\n",
        "embstore_type=\"faiss\"\n",
        "chunk_size = 1000\n",
        "chunk_overlap = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain.vectorstores.faiss.FAISS at 0x7f0afde5cdc0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_vectorstore(\n",
        "    embeddings=emb_model,\n",
        "    source_directory=SOURCE_DATA,\n",
        "    output_directory=database_path,\n",
        "    emb_store_type=embstore_type,\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    exclude_pages=EXCLUDE_DICT,\n",
        "    additional_docs=add_docs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WPyXW-l24VFg"
      },
      "source": [
        "# Prototypes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "933ywV6FSiC5"
      },
      "source": [
        "## Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu4Fey58rXx9",
        "outputId": "a754067e-4c25-48f3-9dca-7c8469342949"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['40 year old male with newly diagnosed moderate UC and articular extraintestinal manifestations',\n",
              " '70 year old female with newly diagnosed severe UC',\n",
              " '35 year old male with known moderate UC with prior exposure to infliximab but has worsening colitis on endoscopy despite compliance',\n",
              " '60 year old female with newly diagnosed moderate UC with a background of congestive cardiac failure',\n",
              " '38 year old female with newly diagnosed moderate UC and psoriasis',\n",
              " '25 year old pregnant woman with severe distal ulcerative colitis',\n",
              " '56 year old man with moderate to severe ulcerative colitis and ankylosing spondylitis',\n",
              " '38 year old man with severe ulcerative colitis and has lost response to vedolizumab',\n",
              " '28 year old woman who has severe extensive ulcerative colitis and has a history of lymphoma which was treated 4 years ago',\n",
              " '36 year old woman with moderate ulcerative colitis and multiple sclerosis']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(osp.join(DATA_DIR, \"queries\", \"uc.txt\"), \"r\", encoding = \"utf-8-sig\") as f:\n",
        "    test_cases = f.readlines()\n",
        "\n",
        "test_cases = [test_case.rstrip() for test_case in test_cases]\n",
        "test_cases"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2fY1Hyjd5mb7"
      },
      "source": [
        "## Experiment 1: Only Text - Normal Prompt Template - GPT4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5DLq8oet-_Av"
      },
      "source": [
        "### Prompt Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX3gOJr9I3AN",
        "outputId": "5c51fc3d-af94-4586-e9dd-8dffb71b6eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
            "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC). Perform the following step\n",
            "\n",
            "ANALYSE the given patient profile based on given query based on one of the following criteria:\n",
            "- Freshly treated patient or patient under maintenance\n",
            "- Prior response to Infliximab\n",
            "- Prior failure to Anti-TNF agents\n",
            "- Prior failure to Vedolizumab\n",
            "- Age\n",
            "- Pregnancy\n",
            "- Extraintestinale manifestations\n",
            "- Pouchitis\n",
            "\n",
            "FINALLY RETURN up to 2 TOP choices of biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
            "\n",
            "Summaries\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"drug_name\": {\"title\": \"Drug Name\", \"description\": \"Name of the drug\", \"type\": \"string\"}, \"advantages\": {\"title\": \"Advantages\", \"description\": \"Advantages of the drug \", \"type\": \"string\"}, \"disadvantages\": {\"title\": \"Disadvantages\", \"description\": \"Disadvantages of the drug\", \"type\": \"string\"}}, \"required\": [\"drug_name\", \"advantages\", \"disadvantages\"]}\n",
            "```\n",
            "\n",
            "Question: User Query\n",
            "Answer:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### STANDARD PROMPT TEMPLATE\n",
        "drug_parser = PydanticOutputParser(pydantic_object=DrugOutput)\n",
        "\n",
        "prompt_template = \"\"\"Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
        "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC). Perform the following step\n",
        "\n",
        "ANALYSE the given patient profile based on given query based on one of the following criteria:\n",
        "- Freshly treated patient or patient under maintenance\n",
        "- Prior response to Infliximab\n",
        "- Prior failure to Anti-TNF agents\n",
        "- Prior failure to Vedolizumab\n",
        "- Age\n",
        "- Pregnancy\n",
        "- Extraintestinale manifestations\n",
        "- Pouchitis\n",
        "\n",
        "FINALLY RETURN up to 2 TOP choices of biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
        "\n",
        "{summaries}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "TEST_PROMPT_TEMPLATE_1 = PromptTemplate(\n",
        "    template = prompt_template,\n",
        "    input_variables = [\"summaries\", \"question\"],\n",
        "    partial_variables={\"format_instructions\": drug_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "print(TEST_PROMPT_TEMPLATE_1.format(summaries = \"Summaries\", question = \"User Query\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aqUPpf1JDd1"
      },
      "source": [
        "### Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Xd_LvsydJzdp"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "llm_type = \"gpt-4\"\n",
        "description = \"normal_prompt_1000_200\"\n",
        "emb_store_dir = os.path.join(EMBSTORE_DIR, \"v1_1000_50\")\n",
        "max_tokens = 1024\n",
        "time = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
        "verbose = True\n",
        "save_path = osp.join(ARTIFACT_DIR, f\"{llm_type}_{description}_{time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-tAQR6i7BVw"
      },
      "outputs": [],
      "source": [
        "# Create and run experiment\n",
        "exp1 = QuestionAnsweringWithIndexSearchExperiment(\n",
        "    prompt_template = TEST_PROMPT_TEMPLATE_1,\n",
        "    vector_store = emb_store_dir,\n",
        "    llm_type = llm_type,\n",
        "    max_tokens = max_tokens,\n",
        "    gt = osp.join(DATA_DIR, \"queries\", \"uc_gt.csv\"),\n",
        "    verbose = verbose\n",
        ")\n",
        "\n",
        "exp1.run_test_cases(test_cases)\n",
        "\n",
        "# Save Output\n",
        "exp1.save_json(save_path+\".json\")\n",
        "exp1.write_csv(save_path+\".csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KHu-zQcL-26D"
      },
      "source": [
        "## Experiment 2: Only Text - CHAT Prompt Template - GPT4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5EPzgAOO_Jkp"
      },
      "source": [
        "### Prompt 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3spHEtJ06cce",
        "outputId": "1655a9f6-4ec5-488c-93ff-e9d33a6aa75f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: \n",
            "Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
            "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC).\n",
            "\n",
            "ANALYSE the given patient profile based on given query based on one of the following criteria:\n",
            "- Whether treated patient is new patient or patient under maintenance\n",
            "- Prior response to Infliximab\n",
            "- Prior failure to Anti-TNF agents\n",
            "- Prior failure to Vedolizumab\n",
            "- Age\n",
            "- Pregnancy\n",
            "- Extraintestinale manifestations\n",
            "- Pouchitis\n",
            "\n",
            "FINALLY RETURN up to 2 TOP choices of biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
            "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
            "\n",
            "Summaries\n",
            "\n",
            "\n",
            "Human: User Query\n"
          ]
        }
      ],
      "source": [
        "### CHAT PROMTP TEMPLATE\n",
        "system_prompt = \"\"\"\n",
        "Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
        "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC).\n",
        "\n",
        "ANALYSE the given patient profile based on given query based on one of the following criteria:\n",
        "- Whether treated patient is new patient or patient under maintenance\n",
        "- Prior response to Infliximab\n",
        "- Prior failure to Anti-TNF agents\n",
        "- Prior failure to Vedolizumab\n",
        "- Age\n",
        "- Pregnancy\n",
        "- Extraintestinale manifestations\n",
        "- Pouchitis\n",
        "\n",
        "FINALLY RETURN up to 2 TOP choices of biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
        "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
        "\n",
        "{summaries}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "TEST_PROMPT_TEMPLATE_2 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(system_prompt, input_variables = [\"summaries\"]),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(TEST_PROMPT_TEMPLATE_2.format(summaries = \"Summaries\", question = \"User Query\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do9b9jEu_Xr-"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "llm_type = \"gpt-4\"\n",
        "description = \"Text_Only_With_CHAT_Prompt\"\n",
        "emb_store_dir = os.path.join(EMBSTORE_DIR, \"v1_1000_50\")\n",
        "max_tokens = 1024\n",
        "time = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
        "verbose = True\n",
        "save_path = osp.join(ARTIFACT_DIR, f\"{llm_type}_{description}_{time}\")\n",
        "print(\"Save directory:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwDbSTHV_Xr-",
        "outputId": "dbb9b128-f1a6-40f7-ee7a-a6523ff32e49"
      },
      "outputs": [],
      "source": [
        "# Create and run experiment\n",
        "exp2 = QuestionAnsweringWithIndexSearchExperiment(\n",
        "    prompt_template = TEST_PROMPT_TEMPLATE_2,\n",
        "    vector_store = emb_store_dir,\n",
        "    llm_type = llm_type,\n",
        "    max_tokens = max_tokens,\n",
        "    gt = osp.join(DATA_DIR, \"queries\", \"uc_gt.csv\"),\n",
        "    verbose = verbose\n",
        ")\n",
        "\n",
        "exp2.run_test_cases(test_cases)\n",
        "\n",
        "# Save Output\n",
        "exp2.save_json(save_path+\".json\")\n",
        "exp2.write_csv(save_path+\".csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: \n",
            "Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
            "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC).\n",
            "\n",
            "Analyse the patient ulcerative colitis (UC) severity and list all risk factors.\n",
            "Analyse the patient profile and list all risk factors. Patient profile includes age, gender, pregnancy status, prior reactions to any drugs, whether the patient is newly diagnosed, extraintestinale manifestation, pouchtitis\n",
            "\n",
            "FINALLY RETURN up to 2 TOP choices of recommended biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
            "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
            "\n",
            "Summaries\n",
            "\n",
            "Human: User Query\n"
          ]
        }
      ],
      "source": [
        "### CHAT PROMTP TEMPLATE\n",
        "system_prompt = \"\"\"\n",
        "Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
        "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC).\n",
        "\n",
        "Analyse the patient ulcerative colitis (UC) severity and list all risk factors.\n",
        "Analyse the patient profile and list all risk factors. Patient profile includes age, gender, pregnancy status, prior reactions to any drugs, whether the patient is newly diagnosed, extraintestinale manifestation, pouchtitis\n",
        "\n",
        "FINALLY RETURN up to 2 TOP choices of recommended biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
        "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
        "\n",
        "{summaries}\n",
        "\"\"\"\n",
        "\n",
        "TEST_PROMPT_TEMPLATE_3 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(system_prompt, input_variables = [\"summaries\"]),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_str = TEST_PROMPT_TEMPLATE_3.format(summaries = \"Summaries\", question = \"User Query\")\n",
        "print(prompt_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save directory: /mnt/c/Users/QUAN/Desktop/medical-chatbot/artifacts/gpt-4_CHAT_Prompt_V2_07-07-2023-17-34-18\n"
          ]
        }
      ],
      "source": [
        "# Settings\n",
        "llm_type = \"gpt-4\"\n",
        "description = \"CHAT_Prompt_V2\"\n",
        "emb_store = \"v5-add-tables_2500_500\"\n",
        "max_tokens = 1024\n",
        "time = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
        "verbose = False\n",
        "save_path = osp.join(ARTIFACT_DIR, f\"{llm_type}_{description}_{time}\")\n",
        "print(\"Save directory:\", save_path)\n",
        "\n",
        "settings = {\n",
        "    \"project\": PROJECT,\n",
        "    \"test_case\": osp.join(DATA_DIR, \"queries\", \"uc.txt\"),\n",
        "    \"prompt\": prompt_str,\n",
        "    \"ground_truth\": \"uc_gt.csv\",\n",
        "    \"description\": description,\n",
        "    \"verbose\": verbose,\n",
        "\n",
        "    \"emb_type\": \"text-embedding-ada-002\",\n",
        "    \"vectorstore\": \"faiss/text-embedding-ada-002/\" + emb_store,\n",
        "    \"chunk_size\": emb_store.split(\"_\")[-2],\n",
        "    \"chunk_overlap\": emb_store.split(\"_\")[-1],\n",
        "    \"additional_docs\": \"data/additional_docs.json\",\n",
        "    \"pinecone_index_name\": None,\n",
        "\n",
        "    \"llm_type\": \"gpt-4\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": max_tokens,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and run experiment\n",
        "exp4 = QuestionAnsweringWithIndexSearchExperiment(\n",
        "    prompt_template = TEST_PROMPT_TEMPLATE_3,\n",
        "    vector_store = os.path.join(EMBSTORE_DIR, emb_store),\n",
        "    llm_type = llm_type,\n",
        "    max_tokens = max_tokens,\n",
        "    gt = osp.join(DATA_DIR, \"queries\", \"uc_gt.csv\"),\n",
        "    verbose = verbose\n",
        ")\n",
        "\n",
        "exp4.run_test_cases(test_cases)\n",
        "\n",
        "# Save Output\n",
        "exp4.save_json(os.path.join(save_path+\"result.json\"))\n",
        "exp4.write_csv(os.path.join(save_path+\"result.csv\"))\n",
        "\n",
        "with open(os.path.join(save_path, \"settings.yaml\"), \"w\") as f:\n",
        "    yaml.dump(settings, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-AGiFA7-VosO"
      },
      "source": [
        "## Experiment 3 - Added Table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
            "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC). Perform the following step\n",
            "\n",
            "ANALYSE the given patient profile based on given query based on one of the following criteria:\n",
            "- Freshly treated patient or patient under maintenance\n",
            "- Prior response to Infliximab\n",
            "- Prior failure to Anti-TNF agents\n",
            "- Prior failure to Vedolizumab\n",
            "- Age\n",
            "- Pregnancy\n",
            "- Extraintestinale manifestations\n",
            "- Pouchitis\n",
            "\n",
            "FINALLY RETURN up to 2 TOP choices of biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
            "\n",
            "Summaries\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"drug_name\": {\"title\": \"Drug Name\", \"description\": \"Name of the drug\", \"type\": \"string\"}, \"advantages\": {\"title\": \"Advantages\", \"description\": \"Advantages of the drug \", \"type\": \"string\"}, \"disadvantages\": {\"title\": \"Disadvantages\", \"description\": \"Disadvantages of the drug\", \"type\": \"string\"}}, \"required\": [\"drug_name\", \"advantages\", \"disadvantages\"]}\n",
            "```\n",
            "\n",
            "Question: User Query\n",
            "Answer:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### STANDARD PROMPT TEMPLATE\n",
        "drug_parser = PydanticOutputParser(pydantic_object=DrugOutput)\n",
        "\n",
        "prompt_template = \"\"\"Make reference to the context given to assess the scenario. If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
        "You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC). Perform the following step\n",
        "\n",
        "ANALYSE the given patient profile based on given query based on one of the following criteria:\n",
        "- Freshly treated patient or patient under maintenance\n",
        "- Prior response to Infliximab\n",
        "- Prior failure to Anti-TNF agents\n",
        "- Prior failure to Vedolizumab\n",
        "- Age\n",
        "- Pregnancy\n",
        "- Extraintestinale manifestations\n",
        "- Pouchitis\n",
        "\n",
        "FINALLY RETURN up to 2 TOP choices of biological drugs given patient profile. Explain the PROS and CONS of the 2 choices.\n",
        "\n",
        "{summaries}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "TEST_PROMPT_TEMPLATE_3 = PromptTemplate(\n",
        "    template = prompt_template,\n",
        "    input_variables = [\"summaries\", \"question\"],\n",
        "    partial_variables={\"format_instructions\": drug_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "print(TEST_PROMPT_TEMPLATE_3.format(summaries = \"Summaries\", question = \"User Query\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "LLM_TYPE = \"gpt-4\"\n",
        "DESCRIPTION = \"With Some Tables\"\n",
        "MAX_TOKENS = 1024\n",
        "TIME = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
        "VERBOSE = True\n",
        "VECTORSTORE = os.path.join(EMBSTORE_DIR, PROJECT, \"faiss\", \"text-embedding-ada-002\", \"v2-add-rows_1000_200\")\n",
        "save_path = osp.join(ARTIFACT_DIR, f\"{LLM_TYPE}_{DESCRIPTION}_{TIME}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and run experiment\n",
        "exp3 = QuestionAnsweringWithIndexSearchExperiment(\n",
        "    prompt_template = TEST_PROMPT_TEMPLATE_3,\n",
        "    vector_store = VECTORSTORE,\n",
        "    llm_type = LLM_TYPE,\n",
        "    max_tokens = MAX_TOKENS,\n",
        "    gt = osp.join(MAIN_DIR, \"ground_truth.csv\"),\n",
        "    verbose = VERBOSE\n",
        ")\n",
        "\n",
        "exp3.run_test_cases(test_cases)\n",
        "\n",
        "# Save Output\n",
        "exp3.save_json(save_path + \".json\")\n",
        "exp3.write_csv(save_path + \".csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4 - Changing to Different Settings\n",
        "\n",
        "- uc-3 - test_case: data/queries/uc.txt, prompt: uc_qa_source_1.py, embstore: faiss/text-embedding-ada-002/v2-add-rows_1000_200\n",
        "- uc-4 - test_case: data/queries/uc.txt, prompt: uc_qa_source_1.py, embstore: faiss/text-embedding-ada-002/v3-add-rows_2500_500\n",
        "- uc-5 - test_case: data/queries/uc.txt, prompt: uc_qa_source_1.py, embstore: faiss/text-embedding-ada-002/v4-add-tables_1000_200\n",
        "- uc-6 - test_case: data/queries/uc.txt, prompt: uc_qa_source_1.py, embstore: faiss/text-embedding-ada-002/v5-add-tables_2500_500\n",
        "- uc-7 - test_case: data/queries/uc.txt, prompt: uc_qa_source_1.py, embstore: faiss/text-embedding-ada-002/v6-add-tables_750_100\n",
        "- uc-8 - test_case: data/queries/uc.txt, prompt: uc_qa_source_2.py, embstore: faiss/text-embedding-ada-002/v5-add-tables_2500_500\n",
        "- uc-1_chat - test_case: data/queries/uc.txt, prompt: uc_qa_source_chat_1.py, embstore: faiss/text-embedding-ada-002/v5-add-tables_2500_500\n",
        "- uc-2_chat - test_case: data/queries/uc.txt, prompt: uc_qa_source_chat_1.py, embstore: faiss/text-embedding-ada-002/v3-add-rows_2500_500\n",
        "- uc-3_chat - test_case: data/queries/uc.txt, prompt: uc_qa_source_chat_1.py, embstore: faiss/text-embedding-ada-002/v4-add-tables_1000_200\n",
        "- uc-4_chat - test_case: data/queries/uc.txt, prompt: uc_qa_source_chat_2.py, embstore: faiss/text-embedding-ada-002/v5-add-tables_2500_500\n",
        "- uc-5_chat - test_case: data/queries/uc_long.txt, prompt: uc_qa_source_chat_3.py, embstore: faiss/text-embedding-ada-002/v5-add-tables_2500_500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! ./src/bash/multi_exps.sh uc_7.yaml uc_8_chat.yaml uc_4_chat.yaml uc_5_chat.yaml"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 5 - QA from all docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Documents into Pages/Text Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of text documents: 58\n",
            "Number of text chunks: 90\n",
            "Number of tokens in all documents: 83628\n",
            "Number of tokens in all text chunks: 88263\n",
            "Max tokens in all documents: 3301\n",
            "Max tokens in all text chunks: 2211\n"
          ]
        }
      ],
      "source": [
        "documents = load_documents(os.path.join(DOCUMENT_SOURCE, PROJECT),\n",
        "                           exclude_pages=EXCLUDE_DICT)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=5000, chunk_overlap=500\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "doc_max_tokens, text_max_tokens = 0, 0\n",
        "\n",
        "print(\"Number of text documents:\", len(documents))\n",
        "print(\"Number of text chunks:\", len(texts))\n",
        "\n",
        "print(\"Number of tokens in all documents:\", check_documents_token(documents))\n",
        "print(\"Number of tokens in all text chunks:\", check_documents_token(texts))\n",
        "\n",
        "for document in documents:\n",
        "    doc_tokens = check_documents_token(document)\n",
        "    if doc_tokens > doc_max_tokens:\n",
        "        doc_max_tokens = doc_tokens\n",
        "\n",
        "for text in texts:\n",
        "    text_tokens = check_documents_token(text)\n",
        "    if text_tokens > text_max_tokens:\n",
        "        text_max_tokens = text_tokens\n",
        "    \n",
        "print(\"Max tokens in all documents:\", doc_max_tokens)\n",
        "print(\"Max tokens in all text chunks:\", text_max_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup Prompts & Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\",\n",
        "                 openai_api_key=OPENAI_KEY)\n",
        "reduce_llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",\n",
        "                        openai_api_key=OPENAI_KEY)\n",
        "collapse_llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",\n",
        "                        openai_api_key=OPENAI_KEY) \n",
        "\n",
        "verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Map\n",
        "system_question_template = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC) using biological drugs.\n",
        "Use the following portion of a long document to see if any of the text is relevant to treatment of given patient profile using biological drugs. \n",
        "Return any relevant text verbatim.\n",
        "______________________\n",
        "{context}\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_question_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"Patient Profile: {question}\"),\n",
        "]\n",
        "CHAT_QUESTION_PROMPT = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "## Reduce\n",
        "\n",
        "system_combine_template = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC) using biological drugs.\n",
        "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "\n",
        "Given the following extracted information of a long document, return up to 2 top choices of biological drugs given the patient profile. \n",
        "Explain the PROS and CONS of the 2 choices with respect to the patient profile.\n",
        "\n",
        "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
        "______________________\n",
        "{summaries}\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_combine_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "]\n",
        "CHAT_COMBINE_PROMPT = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "CHAT_COLLAPSE_PROMPT = CHAT_COMBINE_PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "map_chain = LLMChain(llm=llm, prompt=CHAT_QUESTION_PROMPT, verbose=verbose)\n",
        "reduce_chain = LLMChain(llm=reduce_llm, prompt=CHAT_COMBINE_PROMPT,\n",
        "                        verbose=verbose)\n",
        "collapse_chain = LLMChain(llm=collapse_llm, prompt=CHAT_COLLAPSE_PROMPT,\n",
        "                          verbose=verbose)\n",
        "\n",
        "combine_document_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain,\n",
        "    document_variable_name=\"summaries\",\n",
        "    verbose=verbose\n",
        "    )\n",
        "\n",
        "collapse_document_chain = StuffDocumentsChain(\n",
        "    llm_chain=collapse_chain,\n",
        "    document_variable_name=\"summaries\",\n",
        "    verbose=verbose\n",
        ")\n",
        "\n",
        "map_reduce_qa_chain = MapReduceDocumentsChainV2(\n",
        "    llm_chain=map_chain,\n",
        "    combine_document_chain=combine_document_chain,  \n",
        "    collapse_document_chain=collapse_document_chain,\n",
        "    document_variable_name=\"context\",\n",
        "    combine_max_tokens = 14000,\n",
        "    collapse_max_tokens = 6000,\n",
        "    verbose=verbose,\n",
        "    return_map_steps=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dict = {\n",
        "    \"prompt\":{\n",
        "        \"map\": BaseExperiment.convert_prompt_to_string(map_chain.prompt),\n",
        "        \"combine\": BaseExperiment.convert_prompt_to_string(combine_document_chain.llm_chain.prompt),\n",
        "        \"collapse\": BaseExperiment.convert_prompt_to_string(collapse_document_chain.llm_chain.prompt)\n",
        "    },\n",
        "    \"test_cases\": []\n",
        "}\n",
        "\n",
        "for test_case in test_cases:\n",
        "    answer = map_reduce_qa_chain({\"input_documents\": documents, \"question\": test_case})\n",
        "    output_dict[\"test_cases\"].append(\n",
        "        {\n",
        "            \"question\": answer[\"question\"],\n",
        "            \"answer\": answer[\"output_text\"],\n",
        "            \"intermediate_steps\": answer[\"intermediate_steps\"]\n",
        "        }\n",
        "    )\n",
        "    \n",
        "with open(os.path.join(ARTIFACT_DIR, \"map_reduce_page.json\"), \"w\") as f:\n",
        "    json.dump(output_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Text Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Add tables as texts\n",
        "additional_docs = os.path.join(DATA_DIR, \"additional_docs.json\")\n",
        "\n",
        "with open(additional_docs, \"r\") as f:\n",
        "    add_doc_infos = json.load(f)\n",
        "for add_doc_info in add_doc_infos:\n",
        "    if add_doc_info[\"mode\"] == \"table\":\n",
        "        extra_docs = convert_csv_to_documents(\n",
        "            add_doc_info, concatenate_rows=True\n",
        "            )\n",
        "        texts.extend(extra_docs)\n",
        "        documents.extend(extra_docs)\n",
        "        \n",
        "    elif add_doc_info[\"mode\"] == \"json\":\n",
        "        texts.extend(convert_json_to_documents(add_doc_info))\n",
        "        documents.extend(convert_json_to_documents(add_doc_info))\n",
        "    else:\n",
        "        LOGGER.warning(\n",
        "            \"Invalid document type. No texts added to documents list\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dict = {\n",
        "    \"prompt\":{\n",
        "        \"map\": BaseExperiment.convert_prompt_to_string(map_chain.prompt),\n",
        "        \"combine\": BaseExperiment.convert_prompt_to_string(combine_document_chain.llm_chain.prompt),\n",
        "        \"collapse\": BaseExperiment.convert_prompt_to_string(collapse_document_chain.llm_chain.prompt)\n",
        "    },\n",
        "    \"test_cases\": []\n",
        "}\n",
        "\n",
        "for test_case in test_cases:\n",
        "    answer = map_reduce_qa_chain({\"input_documents\": texts, \"question\": test_case})\n",
        "    output_dict[\"test_cases\"].append(\n",
        "        {\n",
        "            \"question\": answer[\"question\"],\n",
        "            \"answer\": answer[\"output_text\"],\n",
        "            \"intermediate_steps\": answer[\"intermediate_steps\"]\n",
        "        }\n",
        "    )\n",
        "    \n",
        "with open(os.path.join(ARTIFACT_DIR, \"map_reduce_chunk_5000_500.json\"), \"w\") as f:\n",
        "    json.dump(output_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Refine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup Prompts & Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "initial_system_question_template = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC) using biological drugs.\n",
        "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "\n",
        "Given the below context information and no prior knowledge, return up to 2 top choices of biological drugs given the patient profile.\n",
        "Explain the PROS and CONS of the 2 choices with respect to the patient profile.\n",
        "\n",
        "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\"\"\"\n",
        "\n",
        "init_messages = [\n",
        "    SystemMessagePromptTemplate.from_template(initial_system_question_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"Patient Profile: {question}\"),\n",
        "]\n",
        "CHAT_INITIAL_PROMPT = ChatPromptTemplate.from_messages(init_messages)\n",
        "\n",
        "refine_system_question_template = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC) using biological drugs.\n",
        "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "\n",
        "Return up to 2 top choices of biological drugs given the patient profile.\n",
        "Explain the PROS and CONS of the 2 choices with respect to the patient profile.\n",
        "\n",
        "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
        "\"\"\"\n",
        "\n",
        "refine_template = (\n",
        "    \"We have the opportunity to refine the existing answer\"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original answer to better \"\n",
        "    \"answer the question. \"\n",
        "    \"If the context isn't useful, return the original answer.\"\n",
        ")\n",
        "\n",
        "refine_template = \"\"\"\n",
        "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\n",
        "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
        "\"\"\"\n",
        "\n",
        "refine_messages = [\n",
        "    SystemMessagePromptTemplate.from_template(refine_system_question_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"Patient Profile: {question}\"),\n",
        "    AIMessagePromptTemplate.from_template(\"Existing answer: {existing_answer}\"),\n",
        "    HumanMessagePromptTemplate.from_template(refine_template),\n",
        "]\n",
        "CHAT_REFINE_PROMPT = ChatPromptTemplate.from_messages(refine_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "verbose = True\n",
        "\n",
        "init_llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\",\n",
        "                 openai_api_key=OPENAI_KEY)\n",
        "refine_llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\",\n",
        "                        openai_api_key=OPENAI_KEY)\n",
        "\n",
        "initial_chain = LLMChain(llm=llm, prompt=CHAT_INITIAL_PROMPT,verbose=verbose)\n",
        "refine_chain=LLMChain(llm=refine_llm, prompt=CHAT_REFINE_PROMPT, verbose=verbose)\n",
        "\n",
        "refine_qa_chain = RefineDocumentsChain(\n",
        "    initial_llm_chain=initial_chain,\n",
        "    refine_llm_chain=refine_chain,\n",
        "    document_variable_name=\"context_str\",\n",
        "    initial_response_name=\"existing_answer\",\n",
        "    return_intermediate_steps=True,\n",
        "    verbose=verbose,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### On Pages + Extra Documents\n",
        "\n",
        "output_dict = {\n",
        "    \"prompt\":{\n",
        "        \"initial\": BaseExperiment.convert_prompt_to_string(CHAT_INITIAL_PROMPT),\n",
        "        \"refine\": BaseExperiment.convert_prompt_to_string(CHAT_REFINE_PROMPT)\n",
        "    },\n",
        "    \"test_cases\": []\n",
        "}\n",
        "\n",
        "for test_case in test_cases:\n",
        "    answer = map_reduce_qa_chain({\"input_documents\": texts, \"question\": test_case})\n",
        "    output_dict[\"test_cases\"].append(\n",
        "        {\n",
        "            \"question\": answer[\"question\"],\n",
        "            \"answer\": answer[\"output_text\"],\n",
        "            \"intermediate_steps\": answer[\"intermediate_steps\"]\n",
        "        }\n",
        "    )\n",
        "    \n",
        "with open(os.path.join(ARTIFACT_DIR, \"map_reduce_chunk_5000_500.json\"), \"w\") as f:\n",
        "    json.dump(output_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 6: Problem breakdown + Semantic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identify individual patient features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NewLineSeparatedListOutputParser(ListOutputParser):\n",
        "    \"\"\"Parse out comma separated lists.\"\"\"\n",
        "\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return (\n",
        "            \"Your response should be a list of newline separated values, \"\n",
        "            \"eg: `foo\\nbar\\nbaz`\"\n",
        "        )\n",
        "\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "breakdown_system_prompt = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC).\n",
        "If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
        "\n",
        "Analyse the patient profile and medical history and list all related risk factors.\n",
        "- Newly treated patient or patient under maintenance\n",
        "- Prior response/failure to biological drugs (Infliximab, Anti-TNF agents, Vedolizumab, etc)\n",
        "- Age\n",
        "- Pregnancy Status\n",
        "- Extraintestinale manifestations\n",
        "- Pouchitis\n",
        "\n",
        "Return your answer as a list of risk factors.\n",
        "-------------------------\n",
        "Example 1\n",
        "Human:\n",
        "Patient Profile: 36 year old woman with moderate ulcerative colitis and multiple sclerosis\n",
        "\n",
        "AI: \n",
        "- Age: 36 years old female\n",
        "- Medical history: Having multiple sclerosis.\n",
        "\n",
        "Example 2\n",
        "Human: \n",
        "Patient Profile: 40 year old pregnant female with newly diagnosed moderate UC and articular extraintestinal manifestations\n",
        "\n",
        "AI: \n",
        "- Age: 40 years old\n",
        "- Pregnant female\n",
        "- Newly diagnosed \n",
        "- Medical history: Having multiple sclerosis.\n",
        "\n",
        "Example 3:\n",
        "Human:\n",
        "Patient Profile: 35 year old male with known moderate UC with prior exposure to infliximab but has worsening colitis on endoscopy despite compliance\n",
        "\n",
        "AI:\n",
        "- Age: 35 years old male\n",
        "- Medical history: Patient under maintenance treatment of Infliximab with worsening condition despite medication.\n",
        "\"\"\"\n",
        "\n",
        "breakdown_messages = [\n",
        "    SystemMessagePromptTemplate.from_template(breakdown_system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(\"Patient Profile: {question}\")        \n",
        "]\n",
        "\n",
        "BREAKDOWN_PROMPT = ChatPromptTemplate.from_messages(\n",
        "    messages=breakdown_messages,\n",
        "    output_parser=NewLineSeparatedListOutputParser()\n",
        "    )\n",
        "\n",
        "breakdown_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4\",\n",
        "    max_tokens=1024,\n",
        "    openai_api_key=OPENAI_KEY\n",
        ")\n",
        "\n",
        "chain = LLMChain(\n",
        "    prompt=BREAKDOWN_PROMPT,\n",
        "    llm=breakdown_llm,\n",
        "    output_key=\"text_output\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results = {\n",
        "    \"queries\": test_cases,\n",
        "    \"breakdown_output\": [],\n",
        "}\n",
        "\n",
        "for test_case in test_cases:\n",
        "    output = chain({\"question\": test_case})\n",
        "    test_results[\"features_list\"].append(output[\"text_output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run QA on each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_prompt = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC). Make reference to the REFERENCE TEXT given to assess the scenario.\n",
        "\n",
        "Based on the patient profile or medical history, make recommendations of biological drugs for UC treatment. If there are no suitable drugs. just say that \"I don't know\", don't try to make up an answer.\n",
        "For each recommended drug, explain the PROS and CONS in context of the patient profile.\n",
        "Also return drugs which this patient should avoid given his profile or medical history and reasons why the drug should be avoided.\n",
        "\n",
        "=========\n",
        "REFERENCE TEXT:\n",
        "{summaries}\n",
        "\"\"\"\n",
        "\n",
        "human_prompt = \"\"\"\n",
        "=========\n",
        "PATIENT RISK PROFILE: {question}\n",
        "=========\n",
        "\"\"\"\n",
        "\n",
        "FEATURE_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            feature_prompt, input_variables=[\"summaries\"]\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(human_prompt),\n",
        "    ]\n",
        ")\n",
        "\n",
        "verbose = True\n",
        "\n",
        "qa_llm=ChatOpenAI(\n",
        "    model_name=\"gpt-4\", temperature=0, max_tokens=1024,\n",
        "    openai_api_key=OPENAI_KEY, verbose=verbose\n",
        "    )\n",
        "\n",
        "vectorstore = os.path.join(EMBSTORE_DIR, \"v8-add-tables_2500_500\")\n",
        "docsearch = FAISS.load_local(vectorstore, OpenAIEmbeddings(openai_api_key=OPENAI_KEY))\n",
        "\n",
        "qa_from_docs_chain=RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=qa_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": FEATURE_PROMPT_TEMPLATE},\n",
        "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 10}),\n",
        "    return_source_documents=True,\n",
        "    max_tokens_limit=6500,\n",
        "    reduce_k_below_max_tokens=True,\n",
        "    verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_list = []\n",
        "for intermediate_answers in test_results[\"features_list\"]:\n",
        "    output_dict = {}\n",
        "    for intermediate_answer in intermediate_answers:\n",
        "        output = qa_from_docs_chain(intermediate_answer)\n",
        "        output_dict[output[\"question\"]] = output[\"answer\"]\n",
        "    output_list.append(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join(ARTIFACT_DIR, \"intermediate_answers.json\"), \"w\") as f:\n",
        "    json.dump(output_list, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine feature answers into final answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combine_prompt = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC). If you do not know the answer. just say that \"I don't know\", don't try to make up an answer.\n",
        "Make reference to the REFERENCE ANSWERS given to assess the scenario.\n",
        "\n",
        "Combine the answers in REFERENCE ANSWERS to recommend up to 2 TOP choices of biological drugs given patient profile. Explain the PROS and CONS of the choices recommended.\n",
        "Prioritize drugs recommendations based on patients medical conditions and history.\n",
        "Output your answer as a list of JSON objects with keys: drug_name, advantages, disadvantages.\n",
        "=========\n",
        "REFERENCE ANSWERS:\n",
        "{summaries}\n",
        "\n",
        "=========\n",
        "COMBINED ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "COMBINE_PROMPT_TEMPLATE = PromptTemplate.from_template(\n",
        "    template=combine_prompt\n",
        "\n",
        "combine_llm=ChatOpenAI(\n",
        "    model_name=\"gpt-4\",\n",
        "    temperature=0,\n",
        "    max_tokens=1024,\n",
        "    openai_api_key=OPENAI_KEY\n",
        ")\n",
        "\n",
        "combine_chain = LLMChain(\n",
        "    llm=combine_llm,\n",
        "    prompt=COMBINE_PROMPT_TEMPLATE,\n",
        "    output_key=\"output_text\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "summaries = []\n",
        "for intermediate_answers in output_list:\n",
        "    summary = \"\"\n",
        "    for feature, answer in intermediate_answers.items():\n",
        "        summary += f\"Patient Feature: {feature}\\nAnswer: {answer}\\n------------------------\\n\"\n",
        "    summaries.append(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_answers = []\n",
        "for summary in summaries:\n",
        "    output = combine_chain({\"summaries\": summary})\n",
        "    final_answers.append(output[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join(ARTIFACT_DIR, \"final_answers.json\"), \"w\") as f:\n",
        "    json.dump(final_answers, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write Output Answers to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "exp6 = QuestionAnsweringOverDocsExperiment(\n",
        "    gt = os.path.join(DATA_DIR, \"queries\", \"uc_gt.csv\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "exp6.questions = test_cases\n",
        "exp6.answers = final_answers\n",
        "exp6.intermediate_steps = output_list\n",
        "exp6.prompt_map = {\n",
        "    \"breakdown\": BREAKDOWN_PROMPT,\n",
        "    \"feature\": FEATURE_PROMPT_TEMPLATE,\n",
        "    \"combine\": COMBINE_PROMPT_TEMPLATE\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "exp6.save_json(os.path.join(ARTIFACT_DIR, \"output.json\"))\n",
        "exp6.write_csv(os.path.join(ARTIFACT_DIR, \"output.csv\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 7: QA over Summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of text documents: 58\n",
            "Number of text chunks: 90\n",
            "Number of tokens in all documents: 84929\n",
            "Number of tokens in all text chunks: 89564\n"
          ]
        }
      ],
      "source": [
        "documents = load_documents(os.path.join(DOCUMENT_SOURCE, PROJECT),\n",
        "                           exclude_pages=EXCLUDE_DICT)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=5000, chunk_overlap=500\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "doc_max_tokens, text_max_tokens = 0, 0\n",
        "\n",
        "print(\"Number of text documents:\", len(documents))\n",
        "print(\"Number of text chunks:\", len(texts))\n",
        "\n",
        "## Add tables as texts\n",
        "additional_docs = os.path.join(DATA_DIR, \"additional_docs.json\")\n",
        "\n",
        "with open(additional_docs, \"r\") as f:\n",
        "    add_doc_infos = json.load(f)\n",
        "for add_doc_info in add_doc_infos:\n",
        "    if add_doc_info[\"mode\"] == \"table\":\n",
        "        extra_docs = convert_csv_to_documents(\n",
        "            add_doc_info, concatenate_rows=True\n",
        "            )\n",
        "        texts.extend(extra_docs)\n",
        "        documents.extend(extra_docs)\n",
        "        \n",
        "    elif add_doc_info[\"mode\"] == \"json\":\n",
        "        texts.extend(convert_json_to_documents(add_doc_info))\n",
        "        documents.extend(convert_json_to_documents(add_doc_info))\n",
        "    else:\n",
        "        print.warning(\n",
        "            \"Invalid document type. No texts added to documents list\"\n",
        "        )\n",
        "        \n",
        "print(\"Number of tokens in all documents:\", check_documents_token(documents))\n",
        "print(\"Number of tokens in all text chunks:\", check_documents_token(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_summarize_chain(\n",
        "    summarize_prompt: ChatPromptTemplate,\n",
        "    map_llm_type: str = \"gpt-3.5-turbo\",\n",
        "    reduce_llm_type: str = \"gpt-3.5-turbo-16k\",\n",
        "    combine_max_doc_tokens: int = 14000,\n",
        "    collapse_max_doc_tokens: int = 6500,\n",
        "    map_tokens_limit: int = 512,\n",
        "    reduce_tokens_limit: int = 1024,\n",
        "    return_intermediate_steps: bool = True,\n",
        "    verbose: bool = False\n",
        "):\n",
        "\n",
        "    ## LLM & Chains\n",
        "    map_llm = ChatOpenAI(model_name=map_llm_type, temperature=0,\n",
        "                    max_tokens=map_tokens_limit, openai_api_key=OPENAI_KEY)\n",
        "    map_chain = LLMChain(llm=map_llm, prompt=summarize_prompt, verbose=verbose)\n",
        "\n",
        "    reduce_llm = ChatOpenAI(model_name=reduce_llm_type, temperature=0,\n",
        "                        max_tokens=reduce_tokens_limit, openai_api_key=OPENAI_KEY)\n",
        "    reduce_chain = LLMChain(llm=reduce_llm, prompt=summarize_prompt, verbose=verbose)\n",
        "    combine_document_chain = StuffDocumentsChain(\n",
        "        llm_chain=reduce_chain, document_variable_name=\"text\", verbose=verbose\n",
        "    )\n",
        "\n",
        "    collapse_llm = ChatOpenAI(model_name=reduce_llm_type, temperature=0,\n",
        "                        max_tokens=reduce_tokens_limit, openai_api_key=OPENAI_KEY)\n",
        "    collapse_chain = LLMChain(llm=collapse_llm, prompt=summarize_prompt, verbose=verbose)\n",
        "\n",
        "    collapse_document_chain = StuffDocumentsChain(\n",
        "        llm_chain=collapse_chain, document_variable_name=\"text\", verbose=verbose\n",
        "    )\n",
        "\n",
        "    return MapReduceDocumentsChainV2(\n",
        "        llm_chain=map_chain,\n",
        "        combine_document_chain=combine_document_chain,\n",
        "        collapse_document_chain=collapse_document_chain,\n",
        "        document_variable_name=\"text\",\n",
        "        combine_max_tokens=combine_max_doc_tokens,\n",
        "        collapse_max_tokens=collapse_max_doc_tokens,\n",
        "        return_intermediate_steps=return_intermediate_steps,\n",
        "        return_map_steps=return_intermediate_steps\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary: AGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "summarize_system_prompt = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC).\n",
        "Your job is to extract relevant from REFERENCE TEXT and write a summary based on specified requirements.\n",
        "\"\"\"\n",
        "\n",
        "age_summarize_human_prompt = \"\"\"\n",
        "Use the following REFERENCE TEXT to write a summary on recommended drugs treatment for patients based on different age group.\n",
        "For each age group, return the recommended drugs and drugs to avoid, together with the PROS and CONS.\n",
        "If there are no recommendations based on age, just say \"No relevant Information\", do not make up an answer.\n",
        "\n",
        "=========\n",
        "REFERENCE TEXT:\n",
        "\"{text}\"\n",
        "=========\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(summarize_system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(age_summarize_human_prompt)\n",
        "]\n",
        "\n",
        "AGE_SUMMARIZE_PROMPT = ChatPromptTemplate.from_messages(\n",
        "    messages=messages\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided reference text, there is no specific information on recommended drugs for different age groups in the treatment of moderate to severe ulcerative colitis (UC). The text primarily focuses on comparing the efficacy and safety of various agents for induction therapy in biologic-naïve patients and those with prior exposure to tumor necrosis factor (TNF) antagonists. In biologic-naïve patients, infliximab was ranked highest for induction of remission and endoscopic improvement. In patients with prior exposure to TNF antagonists, ustekinumab and tofacitinib were ranked highest. However, the text does not provide any recommendations based on age groups. Therefore, no relevant information is available regarding recommended drugs and drugs to avoid for different age groups in the treatment of moderate to severe UC.\n"
          ]
        }
      ],
      "source": [
        "age_summarize_chain = create_summarize_chain(summarize_prompt = AGE_SUMMARIZE_PROMPT)\n",
        "age_summary = age_summarize_chain({\"input_documents\": documents})\n",
        "\n",
        "print(age_summary[\"output_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary: PREGNANCY STATUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "summarize_system_prompt = \"\"\"You are a physician assistant giving advice on treatment for moderate to severe ulcerative colitis (UC).\n",
        "Your job is to extract relevant from REFERENCE TEXT and write a summary based on specified requirements.\n",
        "\"\"\"\n",
        "\n",
        "pregnancy_summarize_human_prompt = \"\"\"\n",
        "Use the following REFERENCE TEXT to write a summary on recommended drugs treatment for patients based on pregnancy status.\n",
        "In case patient is pregnant, return the recommended drugs and drugs to avoid, together with the PROS and CONS.\n",
        "If there are no recommendations based on pregnancy, just say \"No relevant Information\", do not make up an answer.\n",
        "\n",
        "=========\n",
        "REFERENCE TEXT:\n",
        "\"{text}\"\n",
        "=========\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(summarize_system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(pregnancy_summarize_human_prompt)\n",
        "]\n",
        "\n",
        "PREGNANCY_SUMMARIZE_PROMPT = ChatPromptTemplate.from_messages(\n",
        "    messages=messages\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No relevant information is provided in the reference text regarding the recommended drugs treatment for patients with moderate to severe ulcerative colitis based on pregnancy status.\n"
          ]
        }
      ],
      "source": [
        "pregnancy_summarize_chain = create_summarize_chain(summarize_prompt = PREGNANCY_SUMMARIZE_PROMPT)\n",
        "pregnancy_summary = pregnancy_summarize_chain({\"input_documents\": documents})\n",
        "\n",
        "print(pregnancy_summary[\"output_text\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eZpX-1Ssy6Mw"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 8: Custom Agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3msqfy9mzakH"
      },
      "source": [
        "### CSV Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXicAIYbzk8H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plu6jwcRX0yx"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "model_name = 'text-davinci-003'\n",
        "temperature = 0.0\n",
        "model = OpenAI(model_name=model_name, temperature=temperature, openai_api_key = OPENAI_KEY)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "joke_query = \"Tell me a joke.\"\n",
        "_input = prompt.format_prompt(query=joke_query)\n",
        "\n",
        "output = model(_input.to_string())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
